{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "name": "Copy of tutorial_5_neuromorphic_datasets.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeshraghian/snntorch/blob/master/examples/tutorial_7_neuromorphic_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47d5313e-c29d-4581-a9c7-a45122337069"
      },
      "source": [
        "[<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/snntorch_alpha_w.png?raw=true' width=\"300\">](https://github.com/jeshraghian/snntorch/) \n",
        "[<img src='https://github.com/neuromorphs/tonic/blob/develop/docs/_static/tonic-logo-white.png?raw=true' width=\"200\">](https://github.com/neuromorphs/tonic/)\n",
        "\n",
        "\n",
        "# Neuromorphic Datasets with Tonic + snnTorch\n",
        "## Tutorial 7\n",
        "### By Gregor Lenz (https://lenzgregor.com) and Jason K. Eshraghian (www.jasoneshraghian.com)\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/jeshraghian/snntorch/blob/master/examples/examples/tutorial_7_neuromorphic_datasets.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "[<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/GitHub-Mark-Light-120px-plus.png?raw=true' width=\"28\">](https://github.com/jeshraghian/snntorch/) [<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/GitHub_Logo_White.png?raw=true' width=\"80\">](https://github.com/jeshraghian/snntorch/)"
      ],
      "id": "47d5313e-c29d-4581-a9c7-a45122337069"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oll2NNFeG1NG"
      },
      "source": [
        "The snnTorch tutorial series is based on the following paper. If you find these resources or code useful in your work, please consider citing the following source:\n",
        "\n",
        "> <cite> [Jason K. Eshraghian, Max Ward, Emre Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D. Lu. \"Training Spiking Neural Networks Using Lessons From Deep Learning\". arXiv preprint arXiv:2109.12894, September 2021.](https://arxiv.org/abs/2109.12894) </cite>"
      ],
      "id": "oll2NNFeG1NG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClgsZMOfBVby"
      },
      "source": [
        "# Introduction\n",
        "In this tutorial, you will:\n",
        "* Learn how to load neuromorphic datasets using [Tonic](https://github.com/neuromorphs/tonic)\n",
        "* Make use of caching to speed up dataloading\n",
        "* Train a CSNN with the [Neuromorphic-MNIST](https://tonic.readthedocs.io/en/latest/datasets.html#n-mnist) Dataset\n",
        "\n",
        "If running in Google Colab:\n",
        "* You may connect to GPU by checking `Runtime` > `Change runtime type` > `Hardware accelerator: GPU`\n",
        "* Next, install the latest PyPi distribution of snnTorch and Tonic by clicking into the following cell and pressing `Shift+Enter`."
      ],
      "id": "ClgsZMOfBVby"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDnIEHOKB8LD"
      },
      "source": [
        "!pip install tonic --quiet \n",
        "!pip install snntorch --quiet"
      ],
      "id": "hDnIEHOKB8LD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e93694d9-0f0a-46a0-b17f-c04ac9b73a63"
      },
      "source": [
        "# 1. Using Tonic to Load Neuromorphic Datasets\n",
        "Loading datasets from neuromorphic sensors is made super simple thanks to [Tonic](https://github.com/neuromorphs/tonic), which works much like PyTorch vision.\n",
        "\n",
        "Let's start by loading the neuromorphic version of the MNIST dataset, called [N-MNIST](https://tonic.readthedocs.io/en/latest/reference/datasets.html#n-mnist). We can have a look at some raw events to get a feel for what we're working with."
      ],
      "id": "e93694d9-0f0a-46a0-b17f-c04ac9b73a63"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d286ef9-5fe6-4578-a686-91559a1f81d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "851541fe-6d3d-4319-b4c7-09bb89662944"
      },
      "source": [
        "import tonic\n",
        "\n",
        "dataset = tonic.datasets.NMNIST(save_to='./data', train=True)\n",
        "events, target = dataset[0]\n",
        "print(events)"
      ],
      "id": "7d286ef9-5fe6-4578-a686-91559a1f81d2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(10, 30,    937, 1) (33, 20,   1030, 1) (12, 27,   1052, 1) ...\n",
            " ( 7, 15, 302706, 1) (26, 11, 303852, 1) (11, 17, 305341, 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbnLy107Oo3a"
      },
      "source": [
        "Each row corresponds to a single event, which consists of four parameters: (*x-coordinate, y-coordinate, timestamp, polarity*).\n",
        "\n",
        "* x & y co-ordinates correspond to an address in a $34 \\times 34$ grid.\n",
        "\n",
        "* The timestamp of the event is recorded in microseconds.\n",
        "\n",
        "* The polarity refers to whether an on-spike (+1) or an off-spike (-1) occured; i.e., an increase in brightness or a decrease in brightness.\n",
        "\n",
        "If we were to accumulate those events over time and plot the bins as images, it looks like this:"
      ],
      "id": "IbnLy107Oo3a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6d6a0b0-2a73-4dbe-9576-d06c251f0fa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "c6329a15-30fd-4967-e995-504ae385fe72"
      },
      "source": [
        "tonic.utils.plot_event_grid(events)"
      ],
      "id": "e6d6a0b0-2a73-4dbe-9576-d06c251f0fa4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6bcc031-d11a-4471-b3aa-335eec76d7ad"
      },
      "source": [
        "## 1.1 Transformations\n",
        "\n",
        "However, neural nets don't take lists of events as input. The raw data must be converted into a suitable representation, such as a tensor. We can choose a set of transforms to apply to our data before feeding it to our network. The neuromorphic camera sensor has a temporal resolution of microseconds, which when converted into a dense representation, ends up as a very large tensor. That is why we bin events into a smaller number of frames using the [ToFrame transformation](https://tonic.readthedocs.io/en/latest/reference/transformations.html#frames), which reduces temporal precision but also allows us to work with it in a dense format.\n",
        "\n",
        "* `time_window=1000` integrates events into 1000$~\\mu$s bins\n",
        "\n",
        "* Denoise removes isolated, one-off events. If no event occurs within a neighbourhood of 1 pixel across `filter_time` microseconds, the event is filtered. Smaller `filter_time` will filter more events."
      ],
      "id": "f6bcc031-d11a-4471-b3aa-335eec76d7ad"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30f249be-8a65-4c1c-a21c-d561e904b4bf"
      },
      "source": [
        "import tonic.transforms as transforms\n",
        "\n",
        "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
        "\n",
        "# Denoise removes isolated, one-off events\n",
        "# time_window\n",
        "frame_transform = transforms.Compose([transforms.Denoise(filter_time=10000), \n",
        "                                      transforms.ToFrame(sensor_size=sensor_size, \n",
        "                                                         time_window=1000)\n",
        "                                     ])\n",
        "\n",
        "trainset = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=True)\n",
        "testset = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=False)"
      ],
      "id": "30f249be-8a65-4c1c-a21c-d561e904b4bf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70be0b77-9405-44f6-af49-dfc4d49566c6"
      },
      "source": [
        "## 1.2 Fast Dataloading via Caching\n",
        "\n",
        "The original data is stored in a format that is slow to read. To speed up dataloading, we can make use of disk caching. That means that once files are loaded from the original file, they are written to disk in an efficient format in our cache directory. Let's compare some file reading speeds to read 100 examples."
      ],
      "id": "70be0b77-9405-44f6-af49-dfc4d49566c6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a6bf2a2-ff9f-4cdc-8cb3-02a1a9d71a11"
      },
      "source": [
        "def load_sample_simple():\n",
        "    for i in range(100):\n",
        "        events, target = trainset[i]"
      ],
      "id": "3a6bf2a2-ff9f-4cdc-8cb3-02a1a9d71a11",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a9d3b28-b303-4a17-be78-b9918911a7cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45173976-6093-4a06-cd7e-def89fc5155a"
      },
      "source": [
        "%timeit -o load_sample_simple()"
      ],
      "id": "1a9d3b28-b303-4a17-be78-b9918911a7cd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 2.73 s per loop\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TimeitResult : 1 loop, best of 5: 2.73 s per loop>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b957b32f-d76b-42c0-8c1c-b6b63e84e2ef"
      },
      "source": [
        "We can decrease the time it takes to read 100 samples by using a PyTorch DataLoader in addition to disk caching."
      ],
      "id": "b957b32f-d76b-42c0-8c1c-b6b63e84e2ef"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f47e0798-5259-491a-9d3a-59b13b1b0983"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from tonic import CachedDataset\n",
        "\n",
        "cached_trainset = CachedDataset(trainset, cache_path='./cache/nmnist/train')\n",
        "cached_dataloader = DataLoader(cached_trainset)\n",
        "\n",
        "def load_sample_cached():\n",
        "    for i, (events, target) in enumerate(iter(cached_dataloader)):\n",
        "        if i > 99: break"
      ],
      "id": "f47e0798-5259-491a-9d3a-59b13b1b0983",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17a0219b-4d15-4f0b-b5be-8c728b5e24a9"
      },
      "source": [
        "\n",
        "%timeit -o -r 20 load_sample_cached()"
      ],
      "id": "17a0219b-4d15-4f0b-b5be-8c728b5e24a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3831428d-0511-4fde-84d9-11d08fa45df7"
      },
      "source": [
        "## 1.3 Even Faster DataLoading via Batching\n",
        "\n",
        "Now that we've reduced our loading time, we also want to use batching to make efficient use of the GPU. \n",
        "\n",
        "Because event recordings have different lengths, we are going to provide a  collation function `tonic.collation.PadTensors()` that will pad out shorter recordings to ensure all samples in a batch have the same dimensions. "
      ],
      "id": "3831428d-0511-4fde-84d9-11d08fa45df7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b35c7cd-d292-47cd-9203-7f31aa7f7207"
      },
      "source": [
        "batch_size = 100\n",
        "trainloader = DataLoader(cached_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors())"
      ],
      "id": "5b35c7cd-d292-47cd-9203-7f31aa7f7207",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14b9af4f-141e-4301-8451-445957ec8707"
      },
      "source": [
        "def load_sample_batched():\n",
        "    events, target = next(iter(cached_dataloader))"
      ],
      "id": "14b9af4f-141e-4301-8451-445957ec8707",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dc4b27a-63ac-4edc-94e9-589d548c4769"
      },
      "source": [
        "%timeit -o -r 10 load_sample_batched()"
      ],
      "id": "3dc4b27a-63ac-4edc-94e9-589d548c4769",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a82a7afd-c011-4cd6-ba04-1e7cd438bc1f"
      },
      "source": [
        "By using disk caching and a PyTorch dataloader with multithreading and batching support, we have reduced loading times to less than a tenth per sample in comparison to naively iterating over the dataset!"
      ],
      "id": "a82a7afd-c011-4cd6-ba04-1e7cd438bc1f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ded1bd9-e2f1-479e-899c-c6c2652e6fc9"
      },
      "source": [
        "# 2. Training our network using frames created from events"
      ],
      "id": "2ded1bd9-e2f1-479e-899c-c6c2652e6fc9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9be82d75-69ef-4c1b-ad85-4eca84c73ccf"
      },
      "source": [
        "Now let's actually train a network on the N-MNIST classification task. We start by defining our caching wrappers and dataloaders. While doing that, we're also going to apply some augmentations to the training data. The samples we receive from the cached dataset are frames, so we can make use of PyTorch Vision to apply whatever random transform we would like."
      ],
      "id": "9be82d75-69ef-4c1b-ad85-4eca84c73ccf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ace6cd0b-7b56-4422-b3bd-23bac65db9bd"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "transform = tonic.transforms.Compose([torch.from_numpy,\n",
        "                                      torchvision.transforms.RandomRotation([-10,10])])\n",
        "\n",
        "cached_trainset = CachedDataset(trainset, transform=transform, cache_path='./cache/nmnist/train')\n",
        "\n",
        "# no augmentations for the testset\n",
        "cached_testset = CachedDataset(testset, cache_path='./cache/nmnist/test')\n",
        "\n",
        "batch_size = 128\n",
        "trainloader = DataLoader(cached_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(), shuffle=True)\n",
        "testloader = DataLoader(cached_testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors())"
      ],
      "id": "ace6cd0b-7b56-4422-b3bd-23bac65db9bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "528fe384-a365-4b53-bbfc-ed4dd261d222"
      },
      "source": [
        "A mini-batch now has the dimensions (time steps, batch size, channels, height, width). The number of time steps will be set to that of the longest recording in the mini-batch, and all other samples will be padded with zeros to match it."
      ],
      "id": "528fe384-a365-4b53-bbfc-ed4dd261d222"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9e37337-ad4a-43d5-b429-81a18de5148e"
      },
      "source": [
        "event_tensor, target = next(iter(trainloader))\n",
        "print(event_tensor.shape)"
      ],
      "id": "c9e37337-ad4a-43d5-b429-81a18de5148e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61ae5d4b-2bb3-4191-9f96-04b3c6ba4c41"
      },
      "source": [
        "## 2.1 Defining our network\n",
        "We will use snnTorch + PyTorch to construct a CSNN, just as in the previous tutorial. The convolutional network architecture to be used is: 12C5-MP2-32C5-MP2-800FC10\n",
        "\n",
        "- 12C5 is a 5$\\times$5 convolutional kernel with 12 filters\n",
        "- MP2 is a 2$\\times$2 max-pooling function\n",
        "- 800FC10 is a fully-connected layer that maps 800 neurons to 10 outputs"
      ],
      "id": "61ae5d4b-2bb3-4191-9f96-04b3c6ba4c41"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpKDIkRKUIAB"
      },
      "source": [
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "from snntorch import functional as SF\n",
        "from snntorch import spikeplot as splt\n",
        "import torch.nn as nn"
      ],
      "id": "HpKDIkRKUIAB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "107cb645-0227-4290-9e1b-25d6ae7eac87"
      },
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# neuron and simulation parameters\n",
        "spike_grad = surrogate.fast_sigmoid(slope=75)\n",
        "beta = 0.5\n",
        "\n",
        "#  Initialize Network\n",
        "net = nn.Sequential(nn.Conv2d(2, 12, 5),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "                    nn.Conv2d(12, 32, 5),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "                    nn.Flatten(),\n",
        "                    nn.Linear(32*5*5, 10),\n",
        "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
        "                    ).to(device)"
      ],
      "id": "107cb645-0227-4290-9e1b-25d6ae7eac87",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPFvlqOGi_uW"
      },
      "source": [
        "# this time, we won't return membrane as we don't need it \n",
        "\n",
        "def forward_pass(net, data):  \n",
        "  spk_rec = []\n",
        "  utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
        "\n",
        "  for step in range(data.size(0)):  # data.size(0) = number of time steps\n",
        "      spk_out, mem_out = net(data[step])\n",
        "      spk_rec.append(spk_out)\n",
        "  \n",
        "  return torch.stack(spk_rec)"
      ],
      "id": "zPFvlqOGi_uW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23569dfc-e4a7-490f-8a68-c9ade5e03028"
      },
      "source": [
        "## 2.2 Training\n",
        "\n",
        "In the previous tutorial, Cross Entropy Loss was applied to the total spike count to maximize the number of spikes from the correct class.\n",
        "\n",
        "Another option from the `snn.functional` module is to specify the target number of spikes from correct and incorrect classes. The approach below uses the *Mean Square Error Spike Count Loss*, which aims to elicit spikes from the correct class 80\\% of the time, and 20\\% of the time from incorrect classes. Encouraging incorrect neurons to fire could be motivated to avoid dead neurons."
      ],
      "id": "23569dfc-e4a7-490f-8a68-c9ade5e03028"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VocYbtD7Vwp7"
      },
      "source": [
        "optimizer = torch.optim.Adam(net.parameters(), lr=2e-2, betas=(0.9, 0.999))\n",
        "loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)"
      ],
      "id": "VocYbtD7Vwp7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xkKLsqnmzcw"
      },
      "source": [
        "Training neuromorphic data is expensive as it requires sequentially iterating through many time steps (approximately 300 time steps in the N-MNIST dataset). The following simulation will take some time, so we will just stick to training across 50 iterations (which is roughly 1/10th of a full epoch). Feel free to change `num_iters` if you have more time to kill. As we are printing results at each iteration, the results will be quite noisy and will also take some time before we start to see any sort of improvement.\n",
        "\n",
        "In our own experiments, it took about 20 iterations before we saw any improvement, and after 50 iterations, managed to crack ~60% accuracy. \n",
        "\n",
        "> Warning: the following simulation will take a while. Go make yourself a coffee, or ten. "
      ],
      "id": "7xkKLsqnmzcw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4GbPSdTUcUR"
      },
      "source": [
        "num_iters = 50\n",
        "\n",
        "loss_hist = []\n",
        "acc_hist = []\n",
        "\n",
        "# training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (data, targets) in enumerate(iter(trainloader)):\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        net.train()\n",
        "        spk_rec = forward_pass(net, data)\n",
        "        loss_val = loss_fn(spk_rec, targets)\n",
        "\n",
        "        # Gradient calculation + weight update\n",
        "        optimizer.zero_grad()\n",
        "        loss_val.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Store loss history for future plotting\n",
        "        loss_hist.append(loss_val.item())\n",
        " \n",
        "        print(f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}\")\n",
        "\n",
        "        acc = SF.accuracy_rate(spk_rec, targets) \n",
        "        acc_hist.append(acc)\n",
        "        print(f\"Accuracy: {acc * 100:.2f}%\\n\")\n",
        "\n",
        "        if i == num_iters:\n",
        "          break"
      ],
      "id": "R4GbPSdTUcUR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVjUzNcX0wld"
      },
      "source": [
        "# 3. Results\n",
        "## 3.1 Plot Test Accuracy"
      ],
      "id": "YVjUzNcX0wld"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yp2aTX2_1zFG"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot Loss\n",
        "fig = plt.figure(facecolor=\"w\")\n",
        "plt.plot(acc_hist)\n",
        "plt.title(\"Train Set Accuracy\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()"
      ],
      "id": "yp2aTX2_1zFG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gb1wCQb2bMd"
      },
      "source": [
        "## 3.2 Spike Counter\n",
        "\n",
        "Run a forward pass on a batch of data to obtain spike recordings."
      ],
      "id": "1gb1wCQb2bMd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLAfvj9D2AYd"
      },
      "source": [
        "spk_rec = forward_pass(net, data)"
      ],
      "id": "qLAfvj9D2AYd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQnj40YC2hUV"
      },
      "source": [
        "Changing `idx` allows you to index into various samples from the simulated minibatch. Use `splt.spike_count` to explore the spiking behaviour of a few different samples. Generating the following animation will take some time.\n",
        "\n",
        "> Note: if you are running the notebook locally on your desktop, please uncomment the line below and modify the path to your ffmpeg.exe\n",
        "\n"
      ],
      "id": "VQnj40YC2hUV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTKhuyk22M57"
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "idx = 0\n",
        "\n",
        "fig, ax = plt.subplots(facecolor='w', figsize=(12, 7))\n",
        "labels=['0', '1', '2', '3', '4', '5', '6', '7', '8','9']\n",
        "print(f\"The target label is: {targets[idx]}\")\n",
        "\n",
        "# plt.rcParams['animation.ffmpeg_path'] = 'C:\\\\path\\\\to\\\\your\\\\ffmpeg.exe'\n",
        "\n",
        "#  Plot spike count histogram\n",
        "anim = splt.spike_count(spk_rec[:, idx].detach().cpu(), fig, ax, labels=labels, \n",
        "                        animate=True, interpolate=1)\n",
        "\n",
        "HTML(anim.to_html5_video())\n",
        "# anim.save(\"spike_bar.mp4\")"
      ],
      "id": "oTKhuyk22M57",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iSGTq0Q3Lcm"
      },
      "source": [
        "# Conclusion\n",
        "If you made it this far, then congratulations - you have the patience of a monk. You should now also understand how to load neuromorphic datasets using Tonic and then train a network using snnTorch. [In the next tutorial](https://snntorch.readthedocs.io/en/latest/tutorials/index.html), we will learn more advanced techniques, such as introducing long-term temporal dynamics into our SNNs.\n"
      ],
      "id": "-iSGTq0Q3Lcm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-K_DUnsMKnv"
      },
      "source": [
        "# Additional Resources\n",
        "* [Check out the snnTorch GitHub project here.](https://github.com/jeshraghian/snntorch)\n",
        "* [The Tonic GitHub project can be found here.](https://github.com/neuromorphs/tonic)\n",
        "* The N-MNIST Dataset was originally published in the following paper: [Orchard, G.; Cohen, G.; Jayawant, A.; and Thakor, N.  â€œConverting Static Image Datasets to Spiking Neuromorphic Datasets Using Saccades\", Frontiers in Neuroscience, vol.9, no.437, Oct. 2015.](https://www.frontiersin.org/articles/10.3389/fnins.2015.00437/full) \n",
        "* For further information about how N-MNIST was created, please refer to [Garrick Orchard's website here.](https://www.garrickorchard.com/datasets/n-mnist)"
      ],
      "id": "h-K_DUnsMKnv"
    }
  ]
}